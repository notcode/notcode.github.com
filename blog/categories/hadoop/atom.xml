<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: hadoop | Not Code]]></title>
  <link href="http://notcode.github.io/blog/categories/hadoop/atom.xml" rel="self"/>
  <link href="http://notcode.github.io/"/>
  <updated>2013-10-13T21:29:17+08:00</updated>
  <id>http://notcode.github.io/</id>
  <author>
    <name><![CDATA[coder]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Be careful with mapreduce while writing into database]]></title>
    <link href="http://notcode.github.io/blog/2013/10/13/write-into-database-in-mapreduce-job/"/>
    <updated>2013-10-13T10:04:00+08:00</updated>
    <id>http://notcode.github.io/blog/2013/10/13/write-into-database-in-mapreduce-job</id>
    <content type="html"><![CDATA[<h3>A Strange Case</h3>

<p>Recently we come across a problem when writing into database in mapreduce jobs.
The job gets lots of failure attempt, though succeeds finally.
We reviewed the source code of job, nothing seemed suspicious, there must be something wrong in our cluster.</p>

<p>So we reproduce the scenario.</p>

<!-- more -->


<h3>Crime scene investigate</h3>

<h5>Let&rsquo;s check the log at first.</h5>

<p>From the web console of jobtracker, we find some error logs emitted by the failure attempts.
The logs tell that inserting operation can&rsquo;t be commited because there is already a row in the table, with same primary key.</p>

<h5>What about the job side?</h5>

<p>The job writing into database is a map-only job, and the written table has a primary key on multiple columns.
The input data is guaranted unique on the primary key, so there should be no conflits while executing sql like <code>'insert into ...'</code></p>

<p>We notice that all of the failure attempts&rsquo;s id is greater than 0, that remind me a feature called peculative task in hadoop.
The insert confliction is caused by this feature.</p>

<p>Bingo! We find the killer!</p>

<h3>Close the case</h3>

<p>After close speculative task by <code>job.setMapSpeculativeExecution(false)</code>, the job succeeds silently.</p>

<p>How lucky that the table is created with a primary key!
Otherwise, we will get multiple occurence of the same record silently.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Replica strategy in hdfs is not good enough]]></title>
    <link href="http://notcode.github.io/blog/2013/09/07/replica-strategy-in-hdfs-is-not-good/"/>
    <updated>2013-09-07T19:02:00+08:00</updated>
    <id>http://notcode.github.io/blog/2013/09/07/replica-strategy-in-hdfs-is-not-good</id>
    <content type="html"><![CDATA[<h3>Two methods of Replica</h3>

<p>I&rsquo;ve known two methods of replica in distributed storage system.
One is like hadoop, which distributes the three replicas of a block on three different nodes.
The other is like mongodb, which store data into shardings with three nodes in each sharding.These three nodes in the same sharding are identical.</p>

<p>Both of the two strategy has a &ldquo;at most two dead node&rdquo; tolerance,that meanings if one or two node in cluster crash,all the data is still available. But what happen to three nodes?</p>

<!-- more -->


<h3>Think it for a while</h3>

<p>Suppose there are N nodes and B blocks in cluster. What&rsquo;s the probability of availability with three nodes lost?</p>

<ul>
<li>probability in hadoop cluster</li>
</ul>


<p>For one block,the probability that not all three replicas of this block is on the three dead nodes is:</p>

<pre><code>1-6/[n(n-1)(n-2)]
</code></pre>

<p>The cluster is available with three node lost means all block is available,and the probability is :</p>

<pre><code>(1-6/[n(n-1)(n-2)])^b
</code></pre>

<ul>
<li>probability in mongodb cluster</li>
</ul>


<p>The nodes number is N, shardings number should be N/3. The probability that cluster is not available is 1/[(N/3)(N/3)],
and the opposite is :</p>

<pre><code>1-1/[(N/3)(N/3)]
</code></pre>

<h3>Which is better</h3>

<p>  If N=20,B=10k,which is better?</p>

<ul>
<li>probability in hadoop is: 0.0154%</li>
<li>probability in mongo is: 99.7%</li>
</ul>


<p>See,the tolerance in hadoop is not good enough.</p>

<p>PS: Actually when there are 20 nodes in a cluster,the number of blocks is far greater than 10K.</p>
]]></content>
  </entry>
  
</feed>
